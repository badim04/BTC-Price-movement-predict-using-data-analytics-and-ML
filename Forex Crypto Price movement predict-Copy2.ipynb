{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3adfdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7072fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Define the Problem\n",
    "# Define the problem statement and objectives\n",
    "\n",
    "# Step 2: Gather Data\n",
    "# Load the historical forex data into a pandas DataFrame\n",
    "df = pd.read_csv('forex_data.csv')  # Replace 'forex_data.csv' with your data file path\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "# Perform any necessary data cleaning and transformation\n",
    "# For example, handle missing values or outliers\n",
    "\n",
    "# Step 4: Feature Engineering\n",
    "# Extract or create features relevant for price prediction\n",
    "# Here, we'll use a single feature: Close prices\n",
    "\n",
    "# Step 5: Split Data\n",
    "# Split the data into training, validation, and testing sets\n",
    "train_data, val_data, test_data = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Step 6: Choose an ML Algorithm\n",
    "# Create and compile the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, activation='relu', input_shape=(time_steps, 1)))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Step 7: Train the Model\n",
    "# Train the LSTM model on the training data\n",
    "X_train = train_data['Close'].values.reshape(-1, 1)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Define time steps for the LSTM model\n",
    "time_steps = 10\n",
    "X = []\n",
    "y = []\n",
    "for i in range(time_steps, len(X_train_scaled)):\n",
    "    X.append(X_train_scaled[i - time_steps:i, 0])\n",
    "    y.append(X_train_scaled[i, 0])\n",
    "X_train = np.array(X)\n",
    "y_train = np.array(y)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Step 8: Validate and Tune the Model\n",
    "# Evaluate the model's performance on the validation set\n",
    "X_val = val_data['Close'].values.reshape(-1, 1)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "X_val_processed = []\n",
    "y_val_processed = []\n",
    "for i in range(time_steps, len(X_val_scaled)):\n",
    "    X_val_processed.append(X_val_scaled[i - time_steps:i, 0])\n",
    "    y_val_processed.append(X_val_scaled[i, 0])\n",
    "\n",
    "X_val_processed = np.array(X_val_processed)\n",
    "y_val_processed = np.array(y_val_processed)\n",
    "X_val_processed = np.reshape(X_val_processed, (X_val_processed.shape[0], X_val_processed.shape[1], 1))\n",
    "val_predictions = model.predict(X_val_processed)\n",
    "val_predictions = scaler.inverse_transform(val_predictions)\n",
    "\n",
    "# Step 9: Evaluate the Model\n",
    "# Evaluate the model's performance on the testing set\n",
    "X_test = test_data['Close'].values.reshape(-1, 1)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_test_processed = []\n",
    "y_test_processed = []\n",
    "for i in range(time_steps, len(X_test_scaled)):\n",
    "    X_test_processed.append(X_test_scaled[i - time_steps:i, 0])\n",
    "    y_test_processed.append(X_test_scaled[i, 0])\n",
    "\n",
    "X_test_processed = np.array(X_test_processed)\n",
    "y_test_processed = np.array(y_test_processed)\n",
    "X_test_processed = np.reshape(X_test_processed, (X_test_processed.shape[0], X_test_processed.shape[1], 1))\n",
    "test_predictions = model.predict(X_test_processed)\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "\n",
    "# Step 10: Deploy and Monitor\n",
    "# Integrate the model into a forex trading platform or system for live trading\n",
    "\n",
    "# Step 11: Continuously Improve\n",
    "# Periodically retrain and update the model using new data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf2557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Define the Problem\n",
    "# Define the problem statement and objectives\n",
    "\n",
    "# Step 2: Gather Data\n",
    "# Load the historical forex data into a pandas DataFrame\n",
    "df = pd.read_csv('forex_data.csv')  # Replace 'forex_data.csv' with your data file path\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "# Perform any necessary data cleaning and transformation\n",
    "# For example, handle missing values or outliers\n",
    "\n",
    "# Step 4: Feature Engineering\n",
    "# Extract or create features relevant for price prediction\n",
    "# Here, we'll use the 'Open Price', 'High Price', 'Low Price', 'Close Price', and 'Volume' as features\n",
    "\n",
    "# Extract the features and target variable\n",
    "features = df[['Open Price', 'High Price', 'Low Price', 'Volume']].values\n",
    "target = df['Close Price'].values\n",
    "\n",
    "# Scale the features and target using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "target_scaled = scaler.fit_transform(target.reshape(-1, 1))\n",
    "\n",
    "# Step 5: Split Data\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target_scaled, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Step 6: Choose an ML Algorithm\n",
    "# Create and compile the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Step 7: Train the Model\n",
    "# Train the LSTM model on the training data\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Step 8: Validate and Tune the Model\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_predictions = model.predict(X_val)\n",
    "val_predictions = scaler.inverse_transform(val_predictions)\n",
    "\n",
    "# Step 9: Evaluate the Model\n",
    "# Evaluate the model's performance on the testing set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "\n",
    "# Step 10: Deploy and Monitor\n",
    "# Integrate the model into a forex trading platform or system for live trading\n",
    "\n",
    "# Step 11: Continuously Improve\n",
    "# Periodically retrain and update the model using new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613e2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc197503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Define the Problem\n",
    "# Define the problem statement and objectives\n",
    "\n",
    "# Step 2: Gather Data\n",
    "# Load the historical forex data into a pandas DataFrame\n",
    "df = pd.read_csv('gbpusddaily.csv')  # Replace 'forex_data.csv' with your data file path\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "# Perform any necessary data cleaning and transformation\n",
    "# For example, handle missing values or outliers\n",
    "\n",
    "# Step 4: Feature Engineering\n",
    "# Extract or create features relevant for price prediction\n",
    "# Here, we'll use the 'Close Price' as the feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0a8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'Close Price' as the feature\n",
    "feature = df['Close'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d7d02e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the feature using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "feature_scaled = scaler.fit_transform(feature.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7097c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split Data\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_scaled[:-1], feature_scaled[1:], test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58bd7443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input to match LSTM requirements\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51eae266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Choose an ML Algorithm\n",
    "# Create and compile the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d08d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/34 [==============================] - 3s 15ms/step - loss: 0.3433 - val_loss: 0.5007\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2218 - val_loss: 0.3172\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1135 - val_loss: 0.1521\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0372 - val_loss: 0.0486\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0088 - val_loss: 0.0151\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0049 - val_loss: 0.0105\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0047 - val_loss: 0.0100\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0046 - val_loss: 0.0099\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0045 - val_loss: 0.0092\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.0092\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0042 - val_loss: 0.0089\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0085\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.0080\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0080\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0078\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0036 - val_loss: 0.0069\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 0.0069\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 0.0068\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0032 - val_loss: 0.0059\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0030 - val_loss: 0.0058\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0029 - val_loss: 0.0053\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0028 - val_loss: 0.0050\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0026 - val_loss: 0.0050\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0025 - val_loss: 0.0043\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0024 - val_loss: 0.0045\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 0.0037\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.0037\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0020 - val_loss: 0.0033\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0019 - val_loss: 0.0032\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 9.5147e-04 - val_loss: 9.6834e-04\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 8.9458e-04 - val_loss: 0.0011\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 8.4047e-04 - val_loss: 7.7276e-04\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 7.9123e-04 - val_loss: 7.7806e-04\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 7.4915e-04 - val_loss: 8.0217e-04\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 7.1670e-04 - val_loss: 6.8033e-04\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 6.7945e-04 - val_loss: 5.8872e-04\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 6.4340e-04 - val_loss: 4.6728e-04\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 6.1624e-04 - val_loss: 3.8653e-04\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 5.9166e-04 - val_loss: 4.2331e-04\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 5.6964e-04 - val_loss: 3.5844e-04\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 5.5137e-04 - val_loss: 4.0063e-04\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 5.3419e-04 - val_loss: 3.4486e-04\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 5.1996e-04 - val_loss: 3.1976e-04\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 5.0794e-04 - val_loss: 2.9087e-04\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 4.9686e-04 - val_loss: 2.8034e-04\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 4.8819e-04 - val_loss: 2.5837e-04\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 4.8197e-04 - val_loss: 2.9402e-04\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 4.7279e-04 - val_loss: 2.6634e-04\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 4.6850e-04 - val_loss: 2.6549e-04\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 4.6894e-04 - val_loss: 2.6767e-04\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 4.6047e-04 - val_loss: 2.8175e-04\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 4.5659e-04 - val_loss: 2.7897e-04\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 4.5711e-04 - val_loss: 2.7734e-04\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 4.5417e-04 - val_loss: 2.7554e-04\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 4.5311e-04 - val_loss: 2.7865e-04\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 4.5263e-04 - val_loss: 3.0144e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21d66e08c10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Train the Model\n",
    "# Train the LSTM model on the training data\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62b0ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Validate and Tune the Model\n",
    "# Evaluate the model's performance on the validation set\n",
    "X_val = feature_scaled[-len(X_test) - 1:-1]\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], 1, 1))\n",
    "\n",
    "val_predictions = model.predict(X_val)\n",
    "val_predictions = scaler.inverse_transform(val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ea9193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Evaluate the Model\n",
    "# Evaluate the model's performance on the testing set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adbf1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Deploy and Monitor\n",
    "# Integrate the model into a forex trading platform or system for live trading\n",
    "\n",
    "# Step 11: Continuously Improve\n",
    "# Periodically retrain and update the model using new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c62fe8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Predict the Next 30 Days or Values\n",
    "# Use the trained LSTM model to predict the next 30 days or values\n",
    "\n",
    "# Get the last sequence from the training data\n",
    "last_sequence = feature_scaled[-1:].reshape((1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "674a8182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for the next 30 days or values\n",
    "predictions = []\n",
    "for _ in range(30):\n",
    "    next_prediction = model.predict(last_sequence)\n",
    "    predictions.append(next_prediction[0][0])\n",
    "    last_sequence = np.append(last_sequence[:, :, :], next_prediction.reshape(1, 1, 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a82afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse scale the predicted values\n",
    "predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ad3db30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values for the next 30 days:\n",
      "Day 1: 1.2826992273330688\n",
      "Day 2: 1.422715663909912\n",
      "Day 3: 1.685756802558899\n",
      "Day 4: 2.2584683895111084\n",
      "Day 5: 3.810992479324341\n",
      "Day 6: 8.5302734375\n",
      "Day 7: 21.37932586669922\n",
      "Day 8: 55.10527801513672\n",
      "Day 9: 142.9104766845703\n",
      "Day 10: 372.3188781738281\n",
      "Day 11: 975.294677734375\n",
      "Day 12: 2558.95458984375\n",
      "Day 13: 6717.40966796875\n",
      "Day 14: 17636.5703125\n",
      "Day 15: 46307.71484375\n",
      "Day 16: 121591.421875\n",
      "Day 17: 319268.8125\n",
      "Day 18: 838323.3125\n",
      "Day 19: 2201239.75\n",
      "Day 20: 5779940.0\n",
      "Day 21: 15176774.0\n",
      "Day 22: 39850668.0\n",
      "Day 23: 104638536.0\n",
      "Day 24: 274756416.0\n",
      "Day 25: 721446272.0\n",
      "Day 26: 1894349824.0\n",
      "Day 27: 4974120448.0\n",
      "Day 28: 13060880384.0\n",
      "Day 29: 34294831104.0\n",
      "Day 30: 90050232320.0\n"
     ]
    }
   ],
   "source": [
    "# Print the predicted values for the next 30 days\n",
    "print(\"Predicted values for the next 30 days:\")\n",
    "for i, value in enumerate(predictions):\n",
    "    print(f\"Day {i+1}: {value[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e9568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b095e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv(\"gbpusddaily.csv\")  # Replace with your actual data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b887f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime format\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae45bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Engineering\n",
    "# Extract relevant features from the available columns\n",
    "data[\"Close_Lag1\"] = data[\"Close\"].shift(1)  # Lagged close price feature\n",
    "data[\"Close_Lag2\"] = data[\"Close\"].shift(2)  # Additional lagged close price feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40d6b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more feature engineering code here if needed\n",
    "\n",
    "# Convert date column to numerical representation\n",
    "data[\"NumericalDate\"] = (data[\"Date\"] - data[\"Date\"].min()).dt.days.astype(int)  # Convert to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d53a5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train-Test Split (Time-based)\n",
    "# Split data into training and testing sets based on a specific date\n",
    "split_date = \"2022-01-01\"  # Define the date for splitting the data\n",
    "train_data = data[data[\"Date\"] < split_date]\n",
    "test_data = data[data[\"Date\"] >= split_date]\n",
    "\n",
    "X_train = train_data.drop(columns=[\"Close\"])  # Use all columns except the target (Close price)\n",
    "y_train = train_data[\"Close\"]\n",
    "\n",
    "X_test = test_data.drop(columns=[\"Close\"])\n",
    "y_test = test_data[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd919e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b0b2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f33f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Selection and Training\n",
    "# One-hot encode date column\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "date_encoded = encoder.fit_transform(X_train[[\"Date\"]])\n",
    "feature_names = encoder.get_feature_names_out([\"Date\"])\n",
    "date_encoded_df = pd.DataFrame(date_encoded, columns=feature_names)\n",
    "\n",
    "# Concatenate encoded features with remaining features\n",
    "X_train_encoded = pd.concat([X_train.drop(columns=[\"Date\"]), date_encoded_df], axis=1)\n",
    "\n",
    "# Repeat the same process for the test data\n",
    "date_encoded_test = encoder.transform(X_test[[\"Date\"]])\n",
    "date_encoded_test_df = pd.DataFrame(date_encoded_test, columns=feature_names)\n",
    "X_test_encoded = pd.concat([X_test.drop(columns=[\"Date\"]), date_encoded_test_df], axis=1)\n",
    "\n",
    "# Drop the date column from the training data\n",
    "#X_train_encoded = X_train_encoded.drop(columns=[\"Date\"])\n",
    "\n",
    "# Continue with the remaining steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9edc6a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             0\n",
       "Open             0\n",
       "High             0\n",
       "Low              0\n",
       "Close            0\n",
       "Adj Close        0\n",
       "Volume           0\n",
       "Close_Lag1       1\n",
       "Close_Lag2       2\n",
       "NumericalDate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62cbb9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfe0f4e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_68/196061827.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Random Forest Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_encoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             _assert_all_finite(\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             )\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Step 4: Model Selection and Training\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2200b572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             0\n",
       "Open             0\n",
       "High             0\n",
       "Low              0\n",
       "Close            0\n",
       "Adj Close        0\n",
       "Volume           0\n",
       "Close_Lag1       0\n",
       "Close_Lag2       0\n",
       "NumericalDate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "rf_model.fit(X_train_encoded, y_train)\n",
    "lstm_model.fit(X_train_encoded, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "# Continue with the remaining code for model evaluation\n",
    "\n",
    "# Step 6: Predicting Close Price\n",
    "# Generate predictions for the next 30 days\n",
    "last_sequence = X_test_encoded.iloc[-1:, :]\n",
    "predictions = []\n",
    "\n",
    "for _ in range(30):\n",
    "    last_sequence_scaled = scaler.transform(last_sequence)\n",
    "    last_sequence_reshaped = np.reshape(last_sequence_scaled, (last_sequence_scaled.shape[0], last_sequence_scaled.shape[1], 1))\n",
    "    \n",
    "    next_prediction = lstm_model.predict(last_sequence_reshaped)\n",
    "    next_prediction = scaler.inverse_transform(next_prediction)\n",
    "    \n",
    "    predictions.append(next_prediction[0, 0])\n",
    "    \n",
    "    # Update last_sequence by appending the predicted value\n",
    "    last_sequence = np.append(last_sequence[:, 1:], [[next_prediction]], axis=1)\n",
    "\n",
    "# Step 7: Model Deployment and Monitoring\n",
    "# Print the predicted values for the next 30 days\n",
    "print(\"Predicted values for the next 30 days:\")\n",
    "for i, value in enumerate(predictions):\n",
    "    print(f\"Day {i+1}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2db4cb2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nGradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_68/2301833040.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Gradient Boosting Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_encoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;31m# trees use different types for X and y, checking them separately.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    430\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coo\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             _assert_all_finite(\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             )\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nGradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "gb_model.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ec9a177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Date\nFeature names seen at fit time, yet now missing:\n- Date_2017-01-02T00:00:00.000000000\n- Date_2017-01-03T00:00:00.000000000\n- Date_2017-01-04T00:00:00.000000000\n- Date_2017-01-05T00:00:00.000000000\n- Date_2017-01-06T00:00:00.000000000\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_68/195752903.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_test_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mX_train_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m         X = self._validate_data(\n\u001b[0m\u001b[0;32m    509\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \"\"\"\n\u001b[1;32m--> 548\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"requires_y\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    479\u001b[0m                 )\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m     def _validate_data(\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Date\nFeature names seen at fit time, yet now missing:\n- Date_2017-01-02T00:00:00.000000000\n- Date_2017-01-03T00:00:00.000000000\n- Date_2017-01-04T00:00:00.000000000\n- Date_2017-01-05T00:00:00.000000000\n- Date_2017-01-06T00:00:00.000000000\n- ...\n"
     ]
    }
   ],
   "source": [
    "# LSTM Model\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_reshaped = np.reshape(X_test_scaled, (X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a797b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Model Evaluation\n",
    "# Random Forest Model\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
    "print(\"Random Forest RMSE:\", rf_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ab963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Model\n",
    "gb_predictions = gb_model.predict(X_test)\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_predictions))\n",
    "print(\"Gradient Boosting RMSE:\", gb_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "lstm_predictions = lstm_model.predict(X_test_reshaped)\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions)\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_predictions))\n",
    "print(\"LSTM RMSE:\", lstm_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Selection and Training\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Gradient Boosting Model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# LSTM Model\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_reshaped = np.reshape(X_test_scaled, (X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=16)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "# Random Forest Model\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
    "print(\"Random Forest RMSE:\", rf_rmse)\n",
    "\n",
    "# Gradient Boosting Model\n",
    "gb_predictions = gb_model.predict(X_test)\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_predictions))\n",
    "print(\"Gradient Boosting RMSE:\", gb_rmse)\n",
    "\n",
    "# LSTM Model\n",
    "lstm_predictions = lstm_model.predict(X_test_reshaped)\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions)\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_predictions))\n",
    "print(\"LSTM RMSE:\", lstm_rmse)\n",
    "\n",
    "# Step 6: Predicting Close Price\n",
    "# Generate predictions for the next 30 days\n",
    "last_sequence = X_test.iloc[-1:, :]\n",
    "predictions = []\n",
    "\n",
    "for _ in range(30):\n",
    "    last_sequence_scaled = scaler.transform(last_sequence)\n",
    "    last_sequence_reshaped = np.reshape(last_sequence_scaled, (last_sequence_scaled.shape[0], last_sequence_scaled.shape[1], 1))\n",
    "    \n",
    "    next_prediction = lstm_model.predict(last_sequence_reshaped)\n",
    "    next_prediction = scaler.inverse_transform(next_prediction)\n",
    "    \n",
    "    predictions.append(next_prediction[0, 0])\n",
    "    \n",
    "    # Update last_sequence by appending the predicted value\n",
    "    last_sequence = np.append(last_sequence[:, 1:], [[next_prediction]], axis=1)\n",
    "\n",
    "# Step 7: Model Deployment and Monitoring\n",
    "# Print the predicted values for the next 30 days\n",
    "print(\"Predicted values for the next 30 days:\")\n",
    "for i, value in enumerate(predictions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Predicting Close Price\n",
    "# Generate predictions for the next 30 days\n",
    "last_sequence = X_test.iloc[-1:, :]\n",
    "predictions = []\n",
    "\n",
    "for _ in range(30):\n",
    "    last_sequence_scaled = scaler.transform(last_sequence)\n",
    "    last_sequence_reshaped = np.reshape(last_sequence_scaled, (last_sequence_scaled.shape[0], last_sequence_scaled.shape[1], 1))\n",
    "    \n",
    "    next_prediction = lstm_model.predict(last_sequence_reshaped)\n",
    "    next_prediction = scaler.inverse_transform(next_prediction)\n",
    "    \n",
    "    predictions.append(next_prediction[0, 0])\n",
    "    \n",
    "    # Update last_sequence by appending the predicted value\n",
    "    last_sequence = np.append(last_sequence[:, 1:], [[next_prediction]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Model Deployment and Monitoring\n",
    "# Print the predicted values for the next 30 days\n",
    "print(\"Predicted values for the next 30 days:\")\n",
    "for i, value in enumerate(predictions):\n",
    "    print(f\"Day {i+1}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18358a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21b35051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32257a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv(\"gbpusddaily.csv\")  # Replace with your actual data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d41627ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Engineering\n",
    "# Extract relevant features from the available columns\n",
    "data[\"Close_Lag1\"] = data[\"Close\"].shift(1)  # Lagged close price feature\n",
    "data[\"Close_Lag2\"] = data[\"Close\"].shift(2)  # Additional lagged close price feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba9dbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more feature engineering code here if needed\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "X = data.drop(columns=[\"Close\"])  # Use all columns except the target (Close price)\n",
    "y = data[\"Close\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24737363",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2017-01-02'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4680/1681491255.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Random Forest Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"numpy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"numpy.array_api\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1992\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNpDtype\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '2017-01-02'"
     ]
    }
   ],
   "source": [
    "# Step 4: Model Selection and Training\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c40a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_reshaped = np.reshape(X_test_scaled, (X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Model Evaluation\n",
    "# Random Forest Model\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
    "print(\"Random Forest RMSE:\", rf_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98deb592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Model\n",
    "gb_predictions = gb_model.predict(X_test)\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_predictions))\n",
    "print(\"Gradient Boosting RMSE:\", gb_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d398eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "lstm_predictions = lstm_model.predict(X_test_reshaped)\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions)\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_predictions))\n",
    "print(\"LSTM RMSE:\", lstm_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e312f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Predicting Close Price\n",
    "# Generate predictions for the next 30 days\n",
    "future_features = X_test.iloc[-1:]  # Use the last row of the test set as the starting point for prediction\n",
    "\n",
    "predictions = []\n",
    "for _ in range(30):\n",
    "    future_features_scaled = scaler.transform(future_features)\n",
    "    future_features_reshaped = np.reshape(future_features_scaled, (1, future_features_scaled.shape[1], 1))\n",
    "    \n",
    "    future_prediction = lstm_model.predict(future_features_reshaped)\n",
    "    future_prediction = scaler.inverse_transform(future_prediction)\n",
    "    \n",
    "    predictions.append(future_prediction[0, 0])\n",
    "    \n",
    "    # Update future_features by shifting the predicted value to the next day\n",
    "    future_features = future_features.shift(-1)\n",
    "    future_features.iloc[-1, -1] = future_prediction[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0bf4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Model Deployment and Monitoring\n",
    "# Print the predicted values for the next 30 days\n",
    "print(\"Predicted values for the next 30 days:\")\n",
    "for i, value in enumerate(predictions):\n",
    "    print(f\"Day {i+1}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a6844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
