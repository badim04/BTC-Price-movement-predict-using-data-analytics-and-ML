{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yahoo Finance Library Installation\n",
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97cbd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To fetch financial data\n",
    "import yfinance as yf\n",
    "\n",
    "# For visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc108af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ticker as 'EURUSD=X'\n",
    "forex_data = yf.download('EURUSD=X', start='2019-01-02', end='2021-12-31')\n",
    "\n",
    "# Set the index to a datetime object\n",
    "forex_data.index = pd.to_datetime(forex_data.index)\n",
    "\n",
    "# Display the last five rows\n",
    "forex_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271aaf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the close price\n",
    "plt.figure(figsize=(15, 7))\n",
    "forex_data['Adj Close'].plot()\n",
    "\n",
    "# Set the title and axis label\n",
    "plt.title('EUR/USD Data', fontsize=16)\n",
    "plt.xlabel('Year-Month', fontsize=15)\n",
    "plt.ylabel('Price', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(['Close'], prop={'size': 15})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ticker as 'EURUSD=X'\n",
    "forex_data_minute = yf.download('EURUSD=X', period='5d', interval='1m')\n",
    "\n",
    "# Set the index to a datetime object\n",
    "forex_data_minute.index = pd.to_datetime(forex_data_minute.index)\n",
    "\n",
    "# Display the last five rows\n",
    "forex_data_minute.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787617cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform index type from datetime to string\n",
    "forex_data_minute['dates'] = forex_data_minute.index.strftime(\n",
    "    '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Plot the series\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "ax.plot(forex_data_minute['dates'], forex_data_minute['Adj Close'])\n",
    "\n",
    "# Set title and axis label\n",
    "plt.title('EUR/USD Data', fontsize=16)\n",
    "plt.xlabel('Time', fontsize=15)\n",
    "plt.ylabel('Price', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(['Close'], prop={'size': 15})\n",
    "\n",
    "# Set maximum number of tick locators\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8692ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ticker as 'EURUSD=X'\n",
    "forex_data = yf.download(['EURUSD=X', 'GBPUSD=X'],\n",
    "                         start='2019-01-02', end='2021-12-31', group_by='ticker')\n",
    "\n",
    "# Set the index to a datetime object\n",
    "forex_data.index = pd.to_datetime(forex_data.index)\n",
    "\n",
    "# Display the last five rows\n",
    "forex_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the close price\n",
    "ax = plt.figure(figsize=(15, 7))\n",
    "\n",
    "# Plot both forex pairs\n",
    "ax = forex_data['EURUSD=X']['Close'].plot(label='EUR/USD')\n",
    "ax2 = forex_data['GBPUSD=X']['Close'].plot(secondary_y=True, color='g',  ax=ax, label='GBP/USD')\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title('EUR/USD and GBP/USD Data', fontsize=16)\n",
    "ax.set_xlabel('Year-Month', fontsize=15)\n",
    "ax.set_ylabel('Close Prices', fontsize=15)\n",
    "ax2.set_ylabel('Close Prices', fontsize=15)\n",
    "ax.tick_params(axis='both', labelsize=15)\n",
    "h1, l1 = ax.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax.legend(h1+h2, l1+l2, loc=2, prop={'size': 15})\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Figures/EURUSD_and_GBPUSD_Daily_Data.png', bbox_inches = 'tight')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acecd9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fecd20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def collect_data():\n",
    "    # Collect historical forex prices from a data source.\n",
    "    df = pd.read_csv('data.csv')\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    # Remove duplicate data, correct errors, and transform the data into a format that can be used by machine learning algorithms.\n",
    "    df.drop_duplicates()\n",
    "    df.fillna(0)\n",
    "    df = df.astype(float)\n",
    "    return df\n",
    "\n",
    "def choose_algorithm(df):\n",
    "    # Choose a machine learning algorithm to predict forex prices.\n",
    "    lr = LinearRegression()\n",
    "    return lr\n",
    "\n",
    "def train_algorithm(algorithm, df):\n",
    "    # Train the machine learning algorithm on the historical data.\n",
    "    algorithm.fit(df['x'], df['y'])\n",
    "    return algorithm\n",
    "\n",
    "def test_algorithm(algorithm, df):\n",
    "    # Test the machine learning algorithm on a separate data set.\n",
    "    predictions = algorithm.predict(df['x'])\n",
    "    return predictions\n",
    "\n",
    "def deploy_algorithm(algorithm):\n",
    "    # Deploy the machine learning algorithm to make predictions on real-time forex data.\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = collect_data()\n",
    "    df = clean_data(df)\n",
    "    algorithm = choose_algorithm(df)\n",
    "    algorithm = train_algorithm(algorithm, df)\n",
    "    predictions = test_algorithm(algorithm, df)\n",
    "    deploy_algorithm(algorithm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bbf9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b165ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3adfdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6ad1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92797526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89712b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a CSV file\n",
    "data = pd.read_csv('forex_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc45835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b46f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature Engineering\n",
    "\n",
    "import ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff763df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute technical indicators\n",
    "data['RSI'] = ta.momentum.RSIIndicator(data['close']).rsi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split Data\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d547a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Choose an ML Algorithm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the ML algorithm\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Train the Model\n",
    "\n",
    "# Separate features and target variable\n",
    "train_features = train_data[['RSI']]\n",
    "train_target = train_data['price_movement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeafe667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Validate and Tune the Model\n",
    "\n",
    "# Separate validation features and target variable\n",
    "val_features = val_data[['RSI']]\n",
    "val_target = val_data['price_movement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3509c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = model.score(val_features, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Evaluate the Model\n",
    "\n",
    "# Separate testing features and target variable\n",
    "test_features = test_data[['RSI']]\n",
    "test_target = test_data['price_movement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76845350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the testing set\n",
    "accuracy = model.score(test_features, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Deploy and Monitor\n",
    "\n",
    "# Integrate the model into a forex trading platform or system\n",
    "\n",
    "# Monitor the bot's performance\n",
    "\n",
    "# Step 11: Continuously Improve\n",
    "\n",
    "# Retrain and update the model periodically using new data\n",
    "\n",
    "# Collect feedback and iterate on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17acb13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be48a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8cfdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the Problem\n",
    "\n",
    "# Step 2: Gather Data\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data from a CSV file\n",
    "data = pd.read_csv('forex_data.csv')\n",
    "\n",
    "# Handle missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Step 4: Feature Engineering\n",
    "\n",
    "import ta\n",
    "\n",
    "# Compute technical indicators\n",
    "data['RSI'] = ta.momentum.RSIIndicator(data['close']).rsi()\n",
    "\n",
    "# Step 5: Split Data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "# Step 6: Choose an ML Algorithm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an instance of the ML algorithm\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Step 7: Train the Model\n",
    "\n",
    "# Separate features and target variable\n",
    "train_features = train_data[['RSI']]\n",
    "train_target = train_data['price_movement']\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_features, train_target)\n",
    "\n",
    "# Step 8: Validate and Tune the Model\n",
    "\n",
    "# Separate validation features and target variable\n",
    "val_features = val_data[['RSI']]\n",
    "val_target = val_data['price_movement']\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.score(val_features, val_target)\n",
    "\n",
    "# Step 9: Evaluate the Model\n",
    "\n",
    "# Separate testing features and target variable\n",
    "test_features = test_data[['RSI']]\n",
    "test_target = test_data['price_movement']\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "accuracy = model.score(test_features, test_target)\n",
    "\n",
    "# Step 10: Deploy and Monitor\n",
    "\n",
    "# Integrate the model into a forex trading platform or system\n",
    "\n",
    "# Monitor the bot's performance\n",
    "\n",
    "# Step 11: Continuously Improve\n",
    "\n",
    "# Retrain and update the model periodically using new data\n",
    "\n",
    "# Collect feedback and iterate on the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb3933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ce62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain and update the model periodically using new data\n",
    "\n",
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the existing model\n",
    "model = LogisticRegression()\n",
    "model.load('saved_model.pkl')\n",
    "\n",
    "# Load new data for retraining\n",
    "new_data = pd.read_csv('new_forex_data.csv')\n",
    "\n",
    "# Preprocess the new data\n",
    "# ...\n",
    "\n",
    "# Feature engineering on the new data\n",
    "# ...\n",
    "\n",
    "# Split the new data into features and target variable\n",
    "new_features = new_data[['RSI']]\n",
    "new_target = new_data['price_movement']\n",
    "\n",
    "# Retrain the model with the new data\n",
    "model.fit(new_features, new_target)\n",
    "\n",
    "# Save the updated model\n",
    "model.save('updated_model.pkl')\n",
    "\n",
    "# Evaluate the updated model on a validation or testing set\n",
    "# Load the validation/testing data\n",
    "val_data = pd.read_csv('validation_data.csv')\n",
    "\n",
    "# Preprocess the validation/testing data\n",
    "# ...\n",
    "\n",
    "# Feature engineering on the validation/testing data\n",
    "# ...\n",
    "\n",
    "# Split the validation/testing data into features and target variable\n",
    "val_features = val_data[['RSI']]\n",
    "val_target = val_data['price_movement']\n",
    "\n",
    "# Make predictions using the updated model\n",
    "updated_predictions = model.predict(val_features)\n",
    "\n",
    "# Calculate the accuracy of the updated model\n",
    "updated_accuracy = accuracy_score(val_target, updated_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this code snippet, we assume that you have already trained and saved the initial model using the steps mentioned earlier. Then, you load the saved model, load the new data for retraining, preprocess the new data, and perform feature engineering on the new data.\n",
    "\n",
    "#Next, split the new data into features and the target variable. Use the fit method to retrain the model with the new data. Save the updated model for future use.\n",
    "\n",
    "#To evaluate the updated model, you can load a validation or testing dataset, preprocess and perform feature engineering on it, split it into features and the target variable, and then use the predict method to make predictions. Finally, calculate the accuracy of the updated model by comparing the predicted values with the true values.\n",
    "\n",
    "#Remember to adapt this code according to your specific requirements and the ML algorithm you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17f384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc58eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect feedback for new data\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Define the interval for fetching new data (in seconds)\n",
    "fetch_interval = 1800  # Fetch data every 30 mins\n",
    "\n",
    "while True:\n",
    "    # Fetch new data (replace this with your data fetching mechanism)\n",
    "    new_data = fetch_new_data()\n",
    "\n",
    "    # Save the new data to a file (append to an existing file or create a new one)\n",
    "    with open('new_data.csv', 'a') as file:\n",
    "        new_data.to_csv(file, header=file.tell() == 0, index=False)\n",
    "\n",
    "    # Wait for the next fetch interval\n",
    "    time.sleep(fetch_interval)\n",
    "    \n",
    "    \n",
    "# Collect feedback for new data\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Define the interval for fetching new data (in seconds)\n",
    "fetch_interval = 300  # Fetch data every 5 mins\n",
    "\n",
    "while True:\n",
    "    # Fetch new data (replace this with your data fetching mechanism)\n",
    "    new_data = fetch_new_data()\n",
    "\n",
    "    # Save the new data to a file (append to an existing file or create a new one)\n",
    "    with open('new_data.csv', 'a') as file:\n",
    "        new_data.to_csv(file, header=file.tell() == 0, index=False)\n",
    "\n",
    "    # Wait for the next fetch interval\n",
    "    time.sleep(fetch_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c31661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this code snippet, the fetch_new_data() function represents your mechanism for obtaining new data. You can replace it with the appropriate code to fetch data from an API, database, or any other source.\n",
    "\n",
    "The code opens a file in append mode ('a') and uses the to_csv() method of pandas DataFrame to save the new data to the file. The header argument is set to file.tell() == 0 to write the header row only if the file is empty (i.e., it's the first write operation).\n",
    "\n",
    "The loop continues indefinitely using while True, fetching and saving new data in the specified interval (fetch_interval). You can modify the fetch_interval variable to suit your desired interval for fetching new data.\n",
    "\n",
    "Remember to adapt this code to your specific data fetching mechanism, data format, and file-saving requirements. Additionally, make sure to handle exceptions and errors appropriately to ensure the code runs smoothly and safely.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f71acb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0bf4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a6844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
